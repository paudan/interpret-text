{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of SST-2 Sentences using a 3-Player Introspective Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scrapbook as sb\n",
    "\n",
    "from interpret_text.experimental.introspective_rationale import IntrospectiveRationaleExplainer\n",
    "from interpret_text.experimental.common.preprocessor.glove_preprocessor import GlovePreprocessor\n",
    "from interpret_text.experimental.common.preprocessor.bert_preprocessor import BertPreprocessor\n",
    "from interpret_text.experimental.common.model_config.introspective_rationale_model_config import IntrospectiveRationaleModelConfig\n",
    "from interpret_text.experimental.widget import ExplanationDashboard\n",
    "\n",
    "from notebooks.test_utils.utils_sst2 import load_sst2_pandas_df\n",
    "from notebooks.test_utils.utils_data_shared import load_glove_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we train and evaluate a  [three-player explainer](http://people.csail.mit.edu/tommi/papers/YCZJ_EMNLP2019.pdf) model on a subset of the [SST-2](https://nlp.stanford.edu/sentiment/index.html/) dataset. To run this notebook, we used the SST-2 data files provided [here](https://github.com/AcademiaSinicaNLPLab/sentiment_dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "Here we set some parameters that we use for our modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# if quick run true, skips over embedding, most of model training, and model evaluation; used to quickly test pipeline\n",
    "QUICK_RUN = True\n",
    "MODEL_TYPE = \"RNN\" # currently support either RNN, BERT, or a combination of RNN and BERT\n",
    "CUDA = True\n",
    "\n",
    "# data processing parameters\n",
    "DATA_FOLDER = \"../../../data/sst2\"\n",
    "LABEL_COL = \"labels\" \n",
    "TEXT_COL = \"sentences\"\n",
    "token_count_thresh = 1\n",
    "max_sentence_token_count = 70\n",
    "\n",
    "# training procedure parameters\n",
    "load_pretrained_model = False\n",
    "pretrained_model_path = \"../models/rnn.pth\"\n",
    "MODEL_SAVE_DIR = os.path.join(\"..\", \"models\")\n",
    "model_prefix = \"sst2rnpmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"cuda\": CUDA,\n",
    "    \"model_save_dir\": MODEL_SAVE_DIR, \n",
    "    \"model_prefix\": model_prefix,\n",
    "    \"lr\": 2e-4\n",
    "}\n",
    "\n",
    "if QUICK_RUN:\n",
    "    model_config[\"save_best_model\"] = False\n",
    "    model_config[\"pretrain_cls\"] = True\n",
    "    model_config[\"num_epochs\"] = 1\n",
    "\n",
    "if MODEL_TYPE == \"RNN\":\n",
    "    # (i.e. not using BERT), load pretrained glove embeddings\n",
    "    if not QUICK_RUN:\n",
    "        model_config[\"embedding_path\"] = load_glove_embeddings(DATA_FOLDER)\n",
    "    else:\n",
    "        model_config[\"embedding_path\"] = os.path.join(DATA_FOLDER, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_sst2_pandas_df('train')\n",
    "test_data = load_sst2_pandas_df('test')\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "if QUICK_RUN:\n",
    "    batch_size = 50\n",
    "    train_data = train_data.head(batch_size)\n",
    "    test_data = test_data.head(batch_size)\n",
    "X_train = train_data[TEXT_COL]\n",
    "X_test = test_data[TEXT_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique labels\n",
    "y_labels = all_data[LABEL_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config[\"labels\"] = np.array(sorted(y_labels))\n",
    "model_config[\"num_labels\"] = len(y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and embedding\n",
    "The data is then tokenized and embedded using glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"RNN\":\n",
    "    preprocessor = GlovePreprocessor(token_count_thresh, max_sentence_token_count)\n",
    "    preprocessor.build_vocab(all_data[TEXT_COL])\n",
    "if MODEL_TYPE == \"BERT\":\n",
    "    preprocessor = BertPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to tokenizer output\n",
    "df_train = pd.concat([train_data[LABEL_COL], preprocessor.preprocess(X_train)], axis=1)\n",
    "df_test = pd.concat([test_data[LABEL_COL], preprocessor.preprocess(X_test)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer\n",
    "Then, we create the explainer and train it (or load a pretrained model).\n",
    "The steps involved to set up the explainer: \n",
    "- Initialize explainer\n",
    "- Setup preprocessor for the explainer\n",
    "- Supply necessary model configurations to the explainer\n",
    "- Load the explainer once all necessary modules are setup\n",
    "- Fit/Train the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = IntrospectiveRationaleExplainer(classifier_type=MODEL_TYPE, cuda=CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.build_model_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n"
     ]
    }
   ],
   "source": [
    "explainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 10%|███████████████▎                                                                                                                                         | 1/10 [00:00<00:02,  4.22it/s]\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 6/10 [00:00<00:00, 17.31it/s]\n",
      "  0%|                                                                                                                                                                  | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sda/classify-nlp/interpret-text/python/interpret_text/experimental/introspective_rationale/explainer.py:278\u001b[0m, in \u001b[0;36mIntrospectiveRationaleExplainer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_freeze_classifier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_classifier, entire\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# train the three player model end-to-end\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n",
      "File \u001b[0;32m/mnt/sda/classify-nlp/interpret-text/python/interpret_text/experimental/introspective_rationale/model.py:629\u001b[0m, in \u001b[0;36mIntrospectiveRationaleModel.fit\u001b[0;34m(self, df_train, df_test)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[1;32m    626\u001b[0m     z_baseline \u001b[38;5;241m=\u001b[39m z_baseline\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    628\u001b[0m losses, predict, anti_predict, cls_predict, z, z_rewards \u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_y_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mz_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_m_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m z_batch_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(z_rewards\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_history_rewards\u001b[38;5;241m.\u001b[39mappend(z_batch_reward)\n",
      "File \u001b[0;32m/mnt/sda/classify-nlp/interpret-text/python/interpret_text/experimental/introspective_rationale/model.py:178\u001b[0m, in \u001b[0;36mIntrospectiveRationaleModel._train_one_step\u001b[0;34m(self, X_tokens, label, baseline, mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_G_sup\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_G_rl\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 178\u001b[0m forward_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m predict \u001b[38;5;241m=\u001b[39m forward_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    182\u001b[0m anti_predict \u001b[38;5;241m=\u001b[39m forward_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manti_predict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/sda/classify-nlp/interpret-text/python/interpret_text/experimental/introspective_rationale/model.py:293\u001b[0m, in \u001b[0;36mIntrospectiveRationaleModel.forward\u001b[0;34m(self, X_tokens, X_mask)\u001b[0m\n\u001b[1;32m    283\u001b[0m z_probs_ \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(z_scores_, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    285\u001b[0m z_probs_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m     X_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    291\u001b[0m ) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m X_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m z_probs_)\n\u001b[0;32m--> 293\u001b[0m z, neg_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_rationales\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_probs_\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dimensions (batch_size, length)\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbert_explainers:\n\u001b[1;32m    298\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE_model(X_tokens, X_mask, z)[\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    300\u001b[0m     ]  \u001b[38;5;66;03m# the first output are the logits\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sda/classify-nlp/interpret-text/python/interpret_text/experimental/introspective_rationale/model.py:108\u001b[0m, in \u001b[0;36mIntrospectiveRationaleModel._generate_rationales\u001b[0;34m(self, z_prob_)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_rationales\u001b[39m(\u001b[38;5;28mself\u001b[39m, z_prob_):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m        z_prob_ with dimensions (num_rows, length, 2)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Output:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        z with dimensions (num_rows, length)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     z_prob__ \u001b[38;5;241m=\u001b[39m \u001b[43mz_prob_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dimensions (num_rows * length, 2)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# sample actions\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(z_prob__)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "explainer.fit(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the explainer and measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QUICK_RUN:\n",
    "    explainer.score(df_test)\n",
    "    sparsity = explainer.model.avg_sparsity\n",
    "    accuracy = explainer.model.avg_accuracy\n",
    "    anti_accuracy = explainer.model.avg_anti_accuracy\n",
    "    print(\"Test sparsity: \", sparsity)\n",
    "    print(\"Test accuracy: \", accuracy, \"% Anti-accuracy: \", anti_accuracy)\n",
    "    \n",
    "    # for testing\n",
    "    sb.glue(\"sparsity\", sparsity)\n",
    "    sb.glue(\"accuracy\", accuracy)\n",
    "    sb.glue(\"anti_accuracy\", anti_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local importances\n",
    "We can display the found local importances (the most and least important words for a given sentence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a sentence that needs to be interpreted\n",
    "sentence = \"Beautiful movie ; really good , the popcorn was bad\"\n",
    "s2 = \"a beautiful and haunting examination of the stories we tell ourselves to make sense of the mundane horrors of the world.\"\n",
    "s3 = \"the premise is in extremely bad taste , and the film's supposed insights are so poorly executed and done that even a high school dropout taking his or her first psychology class could dismiss them .\"\n",
    "s4= \"This is a super amazing movie with bad acting\"\n",
    "\n",
    "local_explanation = explainer.explain_local(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize explanations\n",
    "We can visualize local feature importances as a heatmap over words in the document and view importance values of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.visualize(local_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(local_explanation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
